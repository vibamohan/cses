#!/usr/bin/env python3
"""
Competitive Programming Test Runner
===================================

A comprehensive testing tool for competitive programming solutions with advanced diff visualization,
memory monitoring, and HTML report generation.

Features:
- Parallel test execution for faster results
- Memory usage monitoring with configurable limits
- Time limit enforcement with timeout handling
- Word-level diff visualization in HTML reports
- Automatic browser opening for HTML reports
- Plain text log file generation
- Colored terminal output for better readability
- Strict mode for competitive programming compliance

Usage:
    python test_runner.py [OPTIONS]

Basic Examples:
    python test_runner.py                          # Run with default ./solution
    python test_runner.py -e "python solution.py"  # Run Python solution
    python test_runner.py -e "java Solution"       # Run Java solution
    python test_runner.py --html                   # Generate HTML report
    python test_runner.py -q --log                 # Quiet mode with log file

Options:
    -e, --exe COMMAND        Command to run solution (default: ./solution)
    -q, --quiet             Only show failed tests in terminal
    -l, --log               Generate plain text log file
    --html                  Generate HTML report and open in browser
    --no-open               Generate HTML report but don't open browser
    --test-dir DIR          Directory with .in/.out files (default: current)
    --time-limit SECONDS    Per-test time limit (use with --strict)
    --mem-limit MB          Per-test memory limit (use with --strict)
    --strict                Kill tests exceeding limits
    --src-code FILE         Source code file to embed in reports

File Structure:
    Your test directory should contain:
    - 1.in, 1.out    # Test case 1
    - 2.in, 2.out    # Test case 2
    - ...            # Additional test cases
    
    Test files are automatically discovered and sorted numerically.

Advanced Usage:
    # Strict competitive programming mode
    python test_runner.py --strict --time-limit 2.0 --mem-limit 256

    # Full reporting with source code embedding
    python test_runner.py --html --log --src-code solution.cpp

    # Custom test directory with quiet output
    python test_runner.py --test-dir tests/ -q --html

HTML Report Features:
    - Collapsible sections for organized viewing
    - Word-level diff highlighting for failed tests
    - Memory and time statistics for each test
    - Embedded source code display
    - Expand/collapse all functionality
    - Automatic browser opening (can be disabled)

Dependencies:
    - psutil (for memory monitoring): pip install psutil
    - Standard library modules: subprocess, threading, pathlib, etc.

Exit Codes:
    0 - All tests passed
    1 - One or more tests failed

Report Files:
    Generated in .loghtml/ directory:
    - test_report_{testdir}_{timestamp}.html
    - test_log_{testdir}_{timestamp}.txt

NOTE: ALL CODE IN THIS FILE IS GENERATED BY CLAUDE.AI
"""


import argparse
import os
import sys
import subprocess
import time
import threading
import psutil
import difflib
import webbrowser
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Tuple, Optional, Dict, Any
import html
import re

class Colors:
    """ANSI color codes for terminal output"""
    RED = '\033[91m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    MAGENTA = '\033[95m'
    CYAN = '\033[96m'
    WHITE = '\033[97m'
    BOLD = '\033[1m'
    RESET = '\033[0m'

class TestResult:
    """Represents the result of a single test case"""
    def __init__(self, test_num: int, passed: bool, time_taken: float, 
                 memory_used: float, output: str, expected: str, 
                 error: str = "", timeout: bool = False, memory_exceeded: bool = False):
        self.test_num = test_num
        self.passed = passed
        self.time_taken = time_taken
        self.memory_used = memory_used
        self.output = output
        self.expected = expected
        self.error = error
        self.timeout = timeout
        self.memory_exceeded = memory_exceeded

class WordDiffer:
    """Enhanced word-level unified diff generator"""

    @staticmethod
    def tokenize(text: str) -> List[str]:
        """Tokenize text into words and whitespace, preserving structure"""
        tokens = []
        current_token = ""
        in_whitespace = False

        for char in text:
            if char.isspace():
                if not in_whitespace and current_token:
                    tokens.append(current_token)
                    current_token = ""
                current_token += char
                in_whitespace = True
            else:
                if in_whitespace and current_token:
                    tokens.append(current_token)
                    current_token = ""
                current_token += char
                in_whitespace = False

        if current_token:
            tokens.append(current_token)

        return tokens

    @staticmethod
    def generate_unified_diff_html(expected: str, actual: str) -> str:
        """Generate HTML unified diff with word-level granularity"""
        expected_tokens = WordDiffer.tokenize(expected)
        actual_tokens = WordDiffer.tokenize(actual)

        matcher = difflib.SequenceMatcher(None, expected_tokens, actual_tokens)

        unified_html = []

        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            expected_segment = expected_tokens[i1:i2]
            actual_segment = actual_tokens[j1:j2]

            if tag == 'equal':
                for token in expected_segment:
                    unified_html.append(f'<span class="diff-unchanged">{html.escape(token)}</span>')
            elif tag == 'delete':
                for token in expected_segment:
                    if token.strip():
                        unified_html.append(f'<span class="diff-removed">{html.escape(token)}</span>')
                    else:
                        unified_html.append(html.escape(token))
            elif tag == 'insert':
                for token in actual_segment:
                    if token.strip():
                        unified_html.append(f'<span class="diff-added">{html.escape(token)}</span>')
                    else:
                        unified_html.append(html.escape(token))
            elif tag == 'replace':
                for token in expected_segment:
                    if token.strip():
                        unified_html.append(f'<span class="diff-removed">{html.escape(token)}</span>')
                    else:
                        unified_html.append(html.escape(token))
                for token in actual_segment:
                    if token.strip():
                        unified_html.append(f'<span class="diff-added">{html.escape(token)}</span>')
                    else:
                        unified_html.append(html.escape(token))

        return ''.join(unified_html)

class TestRunner:
    def __init__(self, args):
        self.args = args
        self.results: List[TestResult] = []
        self.total_tests = 0
        self.passed_tests = 0
        
    def find_test_files(self) -> List[int]:
        """Find all test input files and return sorted test numbers"""
        test_dir = Path(self.args.test_dir)
        test_files = []
        
        for file in test_dir.glob("*.in"):
            try:
                test_num = int(file.stem)
                out_file = test_dir / f"{test_num}.out"
                if out_file.exists():
                    test_files.append(test_num)
            except ValueError:
                continue
                
        return sorted(test_files)
    
    def run_single_test(self, test_num: int) -> TestResult:
        test_dir = Path(self.args.test_dir)
        input_file = test_dir / f"{test_num}.in"
        output_file = test_dir / f"{test_num}.out"

        try:
            with open(input_file, 'r') as f:
                test_input = f.read()
            with open(output_file, 'r') as f:
                expected_output = f.read()
        except Exception as e:
            return TestResult(test_num, False, 0, 0, "", "", f"Error reading files: {e}")

        start_time = time.time()
        memory_used = 0
        timeout = False
        memory_exceeded = False

        try:
            process = subprocess.Popen(
                self.args.exe.split(),
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )

            kill_event = threading.Event()

            def total_rss(proc: psutil.Process):
                try:
                    return sum(p.memory_info().rss for p in [proc] + proc.children(recursive=True))
                except Exception:
                    return 0

            def monitor_memory():
                nonlocal memory_used, memory_exceeded
                try:
                    proc = psutil.Process(process.pid)
                    while not kill_event.is_set() and process.poll() is None:
                        current_rss = total_rss(proc) / (1024 * 1024)
                        memory_used = max(memory_used, current_rss)
                        if self.args.strict and self.args.mem_limit and memory_used > self.args.mem_limit:
                            memory_exceeded = True
                            kill_event.set()
                            process.kill()
                            break
                        time.sleep(0.005)
                except Exception:
                    pass

            monitor_thread = threading.Thread(target=monitor_memory)
            monitor_thread.daemon = True
            monitor_thread.start()

            timeout_val = self.args.time_limit if self.args.strict and self.args.time_limit else None

            try:
                stdout, stderr = process.communicate(input=test_input, timeout=timeout_val)
            except subprocess.TimeoutExpired:
                timeout = True
                kill_event.set()
                process.kill()
                stdout, stderr = process.communicate()

            kill_event.set()
            monitor_thread.join(timeout=0.1)

            end_time = time.time()
            time_taken = end_time - start_time

            if self.args.strict and self.args.time_limit and time_taken > self.args.time_limit:
                timeout = True

            passed = (
                stdout == expected_output and
                not timeout and
                not memory_exceeded and
                process.returncode == 0
            )

            return TestResult(
                test_num, passed, time_taken, memory_used,
                stdout, expected_output, stderr, timeout, memory_exceeded
            )

        except Exception as e:
            end_time = time.time()
            time_taken = end_time - start_time
            return TestResult(test_num, False, time_taken, memory_used, "", expected_output, str(e))

    
    def run_all_tests(self):
        """Run all test cases"""
        test_numbers = self.find_test_files()
        self.total_tests = len(test_numbers)
        
        if self.total_tests == 0:
            print(f"{Colors.RED}No test files found in {self.args.test_dir}{Colors.RESET}")
            return
        
        print(f"{Colors.BLUE}Running {self.total_tests} tests...{Colors.RESET}")
        
        # Run tests in parallel
        with ThreadPoolExecutor(max_workers=min(4, self.total_tests)) as executor:
            future_to_test = {executor.submit(self.run_single_test, num): num for num in test_numbers}
            
            for future in as_completed(future_to_test):
                result = future.result()
                self.results.append(result)
                
                if result.passed:
                    self.passed_tests += 1
                    if not self.args.quiet:
                        print(f"{Colors.GREEN}✓ Test {result.test_num}: PASSED{Colors.RESET} "
                              f"({result.time_taken:.3f}s, {result.memory_used:.1f}MB)")
                else:
                    status = "FAILED"
                    if result.timeout:
                        status = "TIMEOUT"
                    elif result.memory_exceeded:
                        status = "MEMORY EXCEEDED"
                    
                    print(f"{Colors.RED}✗ Test {result.test_num}: {status}{Colors.RESET} "
                          f"({result.time_taken:.3f}s, {result.memory_used:.1f}MB)")
                    
                    if not self.args.quiet and result.error:
                        print(f"  Error: {result.error}")
        
        # Sort results by test number
        self.results.sort(key=lambda x: x.test_num)
        
        # Print summary
        print(f"\n{Colors.BOLD}Summary:{Colors.RESET}")
        print(f"Tests passed: {Colors.GREEN}{self.passed_tests}{Colors.RESET}/{self.total_tests}")
        
        if self.passed_tests == self.total_tests:
            print(f"{Colors.GREEN}All tests passed!{Colors.RESET}")
        else:
            print(f"{Colors.RED}{self.total_tests - self.passed_tests} tests failed{Colors.RESET}")
    
    def generate_html_report(self):
        """Generate HTML report with word-level diffs"""
        if not self.args.html:
            return
            
        log_dir = Path(".loghtml")
        log_dir.mkdir(exist_ok=True)
        
        # Generate unique filename based on test directory and timestamp
        test_dir_name = Path(self.args.test_dir).resolve().name
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        
        log_file = log_dir / f"test_log_{test_dir_name}_{timestamp}.txt"
        html_file = log_dir / f"test_report_{test_dir_name}_{timestamp}.html"
        
        # Read source code if provided
        source_code = ""
        if self.args.src_code:
            try:
                with open(self.args.src_code, 'r') as f:
                    source_code = f.read()
            except Exception as e:
                source_code = f"Error reading source code: {e}"
        
        # Generate HTML
        html_content = f"""<!DOCTYPE html>
<html>
<head>
    <title>Test Report</title>
    <style>
        body {{ font-family: 'Consolas', 'Monaco', monospace; margin: 20px; }}
        .summary {{ background: #f0f0f0; padding: 15px; border-radius: 5px; margin-bottom: 20px; }}
        .test-case {{ border: 1px solid #ddd; margin: 10px 0; border-radius: 5px; }}
        .test-header {{ padding: 10px; cursor: pointer; background: #f9f9f9; }}
        .test-header:hover {{ background: #f0f0f0; }}
        .test-content {{ padding: 10px; display: none; }}
        .passed {{ border-left: 5px solid #4CAF50; }}
        .failed {{ border-left: 5px solid #f44336; }}
        .timeout {{ border-left: 5px solid #ff9800; }}
        .diff {{ background: #f5f5f5; padding: 10px; border-radius: 3px; overflow-x: auto; }}
        .diff-section {{ margin: 10px 0; }}
        .diff-label {{ font-weight: bold; margin-bottom: 5px; }}
        .diff-content {{ border: 1px solid #ddd; padding: 10px; border-radius: 3px; white-space: pre-wrap; word-wrap: break-word; font-family: 'Courier New', monospace; }}
        .diff-removed {{ background-color: #ffebee; color: #c62828; padding: 1px 2px; border-radius: 2px; font-weight: bold; }}
        .diff-added {{ background-color: #e8f5e8; color: #2e7d32; padding: 1px 2px; border-radius: 2px; font-weight: bold; }}
        .diff-unchanged {{ color: #555; }}
        .source-code {{ background: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto; }}
        .collapsible-section {{ margin: 10px 0; border: 1px solid #e0e0e0; border-radius: 3px; }}
        .collapsible-header {{ 
            background: #f8f9fa; 
            padding: 8px 12px; 
            cursor: pointer; 
            font-weight: bold;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }}
        .collapsible-header:hover {{ background: #e9ecef; }}
        .collapsible-content {{ padding: 10px; display: none; max-height: 400px; overflow-y: auto; }}
        .global-source-code {{ background: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto; }}
        .expand-indicator {{ 
            font-size: 12px;
            transition: transform 0.3s ease;
            color: #666;
            width: 0;
            height: 0;
            border-left: 6px solid #666;
            border-top: 4px solid transparent;
            border-bottom: 4px solid transparent;
            display: inline-block;
            margin-left: 5px;
        }}
        .expand-indicator {{
            border-left: 6px solid #666;
            border-top: 4px solid transparent;
            border-bottom: 4px solid transparent;
        }}
        .expand-indicator.expanded {{
            border-left: 4px solid transparent;
            border-right: 4px solid transparent;
            border-top: none;
            border-bottom: 6px solid #666;
            transform: none;
        }}
        pre {{ margin: 0; }}
        .controls {{ margin-bottom: 20px; }}
        .controls button {{
            margin-right: 10px;
            padding: 8px 16px;
            background: #007bff;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
        }}
        .controls button:hover {{
            background: #0056b3;
        }}
        .controls button:active {{
            transform: translateY(1px);
        }}
    </style>
    <script>
        function toggleTest(id) {{
            var content = document.getElementById(id);
            content.style.display = content.style.display === 'none' ? 'block' : 'none';
        }}
        
        function toggleSection(id) {{
            var content = document.getElementById(id);
            var header = content.previousElementSibling;
            var indicator = header.querySelector('.expand-indicator');
            
            if (content.style.display === 'none' || content.style.display === '') {{
                content.style.display = 'block';
                if (indicator) {{
                    indicator.classList.add('expanded');
                }}
            }} else {{
                content.style.display = 'none';
                if (indicator) {{
                    indicator.classList.remove('expanded');
                }}
            }}
        }}
        
        function expandAll() {{
            var allTests = document.querySelectorAll('.test-content');
            var allSections = document.querySelectorAll('.collapsible-content');
            var allIndicators = document.querySelectorAll('.expand-indicator');
            
            allTests.forEach(function(test) {{
                test.style.display = 'block';
            }});
            
            allSections.forEach(function(section) {{
                section.style.display = 'block';
            }});
            
            allIndicators.forEach(function(indicator) {{
                indicator.classList.add('expanded');
            }});
        }}
        
        function collapseAll() {{
            var allTests = document.querySelectorAll('.test-content');
            var allSections = document.querySelectorAll('.collapsible-content');
            var allIndicators = document.querySelectorAll('.expand-indicator');
            
            allTests.forEach(function(test) {{
                test.style.display = 'none';
            }});
            
            allSections.forEach(function(section) {{
                section.style.display = 'none';
            }});
            
            allIndicators.forEach(function(indicator) {{
                indicator.classList.remove('expanded');
            }});
        }}
    </script>
</head>
<body>
    <h1>Test Report</h1>
    <div class="summary">
        <h2>Summary</h2>
        <p>Tests passed: <strong>{self.passed_tests}/{self.total_tests}</strong></p>
        <p>Command: <code>{html.escape(self.args.exe)}</code></p>
        <p>Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
    
    <div class="controls">
        <button onclick="expandAll()">Expand All</button>
        <button onclick="collapseAll()">Collapse All</button>
    </div>
"""
        
        if source_code:
            html_content += f"""
    <div class="collapsible-section">
        <div class="collapsible-header" onclick="toggleSection('global-source-code')">
            <span>Source Code</span>
            <span class="expand-indicator"></span>
        </div>
        <div class="collapsible-content" id="global-source-code">
            <pre><code>{html.escape(source_code)}</code></pre>
        </div>
    </div>
"""
        
        # Add test cases
        for result in self.results:
            status_class = "passed" if result.passed else ("timeout" if result.timeout else "failed")
            status_text = "PASSED" if result.passed else ("TIMEOUT" if result.timeout else "FAILED")
            
            html_content += f"""
    <div class="test-case {status_class}">
        <div class="test-header" onclick="toggleTest('test-{result.test_num}')">
            <strong>Test {result.test_num}: {status_text}</strong>
            <span style="float: right;">Time: {result.time_taken:.3f}s | Memory: {result.memory_used:.1f}MB</span>
        </div>
        <div class="test-content" id="test-{result.test_num}">
"""
            
            # Add input section (always present)
            test_dir = Path(self.args.test_dir)
            input_file = test_dir / f"{result.test_num}.in"
            try:
                with open(input_file, 'r') as f:
                    input_content = f.read()
            except:
                input_content = "Error reading input file"
            
            html_content += f"""
            <div class="collapsible-section">
                <div class="collapsible-header" onclick="toggleSection('input-{result.test_num}')">
                    <span>Input</span>
                    <span class="expand-indicator"></span>
                </div>
                <div class="collapsible-content" id="input-{result.test_num}">
                    <pre>{html.escape(input_content)}</pre>
                </div>
            </div>
"""
            
            if not result.passed:
                # Generate unified diff
                unified_diff = WordDiffer.generate_unified_diff_html(result.expected, result.output)
                
                html_content += f"""
            <div class="collapsible-section">
                <div class="collapsible-header" onclick="toggleSection('diff-{result.test_num}')">
                    <span>Word-Level Unified Diff</span>
                    <span class="expand-indicator"></span>
                </div>
                <div class="collapsible-content" id="diff-{result.test_num}">
                    <div class="diff">
                        <div class="diff-content">{unified_diff}</div>
                    </div>
                </div>
            </div>
"""
            
            # Expected output section
            html_content += f"""
            <div class="collapsible-section">
                <div class="collapsible-header" onclick="toggleSection('expected-{result.test_num}')">
                    <span>Expected Output</span>
                    <span class="expand-indicator"></span>
                </div>
                <div class="collapsible-content" id="expected-{result.test_num}">
                    <pre>{html.escape(result.expected)}</pre>
                </div>
            </div>
"""
            
            # Actual output section
            html_content += f"""
            <div class="collapsible-section">
                <div class="collapsible-header" onclick="toggleSection('actual-{result.test_num}')">
                    <span>Actual Output</span>
                    <span class="expand-indicator"></span>
                </div>
                <div class="collapsible-content" id="actual-{result.test_num}">
                    <pre>{html.escape(result.output)}</pre>
                </div>
            </div>
"""
            
            if result.error:
                html_content += f"""
            <div class="collapsible-section">
                <div class="collapsible-header" onclick="toggleSection('error-{result.test_num}')">
                    <span>Error</span>
                    <span class="expand-indicator"></span>
                </div>
                <div class="collapsible-content" id="error-{result.test_num}">
                    <pre>{html.escape(result.error)}</pre>
                </div>
            </div>
"""
            
            html_content += """
        </div>
    </div>
"""
        
        html_content += """
</body>
</html>
"""
        
        with open(html_file, 'w') as f:
            f.write(html_content)
        
        print(f"\n{Colors.CYAN}HTML report generated: {html_file}{Colors.RESET}")
        
        # Open HTML file in default browser (unless --no-open is specified)
        if not self.args.no_open:
            try:
                webbrowser.open(f"file://{html_file.resolve()}")
                print(f"{Colors.GREEN}HTML report opened in browser{Colors.RESET}")
            except Exception as e:
                print(f"{Colors.YELLOW}Could not open HTML report automatically: {e}{Colors.RESET}")
                print(f"{Colors.CYAN}Please open manually: {html_file}{Colors.RESET}")
        else:
            print(f"{Colors.CYAN}Use --html without --no-open to open automatically{Colors.RESET}")
    
    def generate_log_file(self):
        """Generate plain text log file"""
        if not self.args.log:
            return
            
        log_dir = Path(".loghtml")
        log_dir.mkdir(exist_ok=True)
        
        # Generate unique filename based on test directory and timestamp
        test_dir_name = Path(self.args.test_dir).resolve().name
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        
        log_file = log_dir / f"test_log_{test_dir_name}_{timestamp}.txt"
        
        with open(log_file, 'w') as f:
            f.write(f"Test Report - {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Command: {self.args.exe}\n")
            f.write(f"Tests passed: {self.passed_tests}/{self.total_tests}\n")
            f.write("=" * 50 + "\n\n")
            
            for result in self.results:
                status = "PASSED" if result.passed else ("TIMEOUT" if result.timeout else "FAILED")
                f.write(f"Test {result.test_num}: {status}\n")
                f.write(f"Time: {result.time_taken:.3f}s | Memory: {result.memory_used:.1f}MB\n")
                
                if not result.passed:
                    f.write("Expected Output:\n")
                    f.write(result.expected)
                    f.write("\nActual Output:\n")
                    f.write(result.output)
                    if result.error:
                        f.write(f"\nError: {result.error}")
                    f.write("\n")
                
                f.write("-" * 30 + "\n")
        
        print(f"{Colors.CYAN}Log file generated: {log_file}{Colors.RESET}")

def main():
    parser = argparse.ArgumentParser(description="Competitive Programming Test Runner")
    parser.add_argument('-e', '--exe', default='./solution', 
                       help='Path or command to run the solution (default: ./solution)')
    parser.add_argument('-q', '--quiet', action='store_true',
                       help='Only show failed tests in terminal output')
    parser.add_argument('-l', '--log', action='store_true',
                       help='Save plain text log to .loghtml/test_log.txt')
    parser.add_argument('--html', action='store_true',
                       help='Generate HTML report in .loghtml/ and open in browser')
    parser.add_argument('--no-open', action='store_true',
                       help='Generate HTML report but do not open in browser automatically')
    parser.add_argument('--test-dir', default='.',
                       help='Directory containing test .in and .out files (default: current directory)')
    parser.add_argument('--time-limit', type=float,
                       help='Per-test time limit in seconds (used with --strict)')
    parser.add_argument('--mem-limit', type=float,
                       help='Per-test memory limit in MB (used with --strict)')
    parser.add_argument('--strict', action='store_true',
                       help='Kill tests exceeding time or memory limits')
    parser.add_argument('--src-code', 
                       help='Path to source code file to embed in HTML and log')
    
    args = parser.parse_args()
    
    # Validate arguments
    if args.strict and not (args.time_limit or args.mem_limit):
        print(f"{Colors.YELLOW}Warning: --strict specified but no limits set{Colors.RESET}")
    
    if not Path(args.test_dir).exists():
        print(f"{Colors.RED}Error: Test directory {args.test_dir} does not exist{Colors.RESET}")
        sys.exit(1)
    
    # Check if psutil is available
    try:
        import psutil
    except ImportError:
        print(f"{Colors.RED}Error: psutil package required for memory monitoring{Colors.RESET}")
        print("Install with: pip install psutil")
        sys.exit(1)
    
    # Run tests
    runner = TestRunner(args)
    runner.run_all_tests()
    runner.generate_html_report()
    runner.generate_log_file()
    
    # Exit with appropriate code
    sys.exit(0 if runner.passed_tests == runner.total_tests else 1)

if __name__ == "__main__":
    main()